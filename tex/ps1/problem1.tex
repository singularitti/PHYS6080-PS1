\section{Problem \thesection}\label{sec:p1}

Here, we will look at the propagation of finite precision errors in
multiplication. We start by truncating the input numbers to a fixed precision, given by
\code{ndig}, and compare this to the result without truncating. To keep things relatively simple,
we start from random numbers distributed uniformly between $0$ and $1$. We then shift these,
to keep the result from getting either too large or too small.

In Snippet \ref{lst:round}, to round a floating point number $x$, it is first decomposed by
\code{math.frexp(x)} and the mantissa is rounded to a desired number of digits. Then the
rounded mantissa is multiplied by the exponent to give a rounded version of the floating
point number.

\begin{algorithm}
    \caption{Rounding a floating point number to a certain number of digits.}
    \label{lst:round}
    \begin{pythoncode}
        def myround(x, ndigits):
            mantissa, exponent = math.frexp(x)
            return round(mantissa, ndigits) * 2**exponent
    \end{pythoncode}
\end{algorithm}

We can also write a similar \code{truncate} function to discard the rest of the digits
in $x$, as shown in Snippet \ref{lst:truncate}.

\begin{algorithm}
    \caption{Truncating a floating point number to a certain number of digits.}
    \label{lst:truncate}
    \begin{pythoncode}
        def truncate(x, ndigits):
            return int(x * 10**ndigits) * 10**(-ndigits)
    \end{pythoncode}
\end{algorithm}

To see how finite precision errors propagates in multiplication, we need first define the
rounded and truncated versions of multiplication, as in Snippet \ref{lst:multiply}:

\begin{algorithm}
    \caption{Rounded and truncated versions of multiplication of floating point numbers.}
    \label{lst:multiply}
    \begin{pythoncode}
        def rounded_mul(x, y, ndigits):
            x, y = myround(x, ndigits), myround(y, ndigits)
            return x * y


        def truncated_mul(x, y, ndigits):
            return x * truncate(y, ndigits)
    \end{pythoncode}
\end{algorithm}

Then we can define the accumulated results of $x$ times $y$, as shown in Snippet
\ref{lst:accumulate}.

\begin{algorithm}
    \caption{Rounded and truncated versions of multiplication of floating point numbers.}
    \label{lst:accumulate}
    \begin{pythoncode}
        def rounded_accumulate(ys, ndigits, initial):
            func = partial(rounded_mul, ndigits=ndigits)
            return reduce(func, ys, initial)


        def truncated_accumulate(ys, ndigits, initial):
            func = partial(truncated_mul, ndigits=ndigits)
            return reduce(func, ys, truncate(initial, ndigits))
    \end{pythoncode}
\end{algorithm}

\Question For this problem, you should start from this code and adapt it to be run many times in a
loop (hundreds of times). For each time through the loop, you should store the fractional
differences for both rounding and truncation after $10$, $100$, $1000$ and $10000$ multiplications.
Plot a histogram of the deviations for both cases and the three different numbers of
multiplications. Plot the average difference versus the number of multiplies for both the
rounding and truncation cases to see how it scales with the number of multiplies.

\Answer Functions \code{rounded_accumulate} and \code{truncated_accumulate} in Snippet
\ref{lst:accumulate} are already wrapped and ready to be run in loops.
The next step to do is to store all the intermediate data as required.
The \code{pandas.DataFrame} is especially suitable for doing such job.
So here we will store the real values and the fractional
differences for both rounding and truncation after $10$, $100$, $1000$ and $10000$
multiplications.
The corresponding code is written in Snippet \ref{lst:compare}.

\begin{algorithm}
    \caption{Storing the real values and the fractional
        differences for both rounding and truncation after $10$, $100$, $1000$ and $10000$
        multiplications.}
    \label{lst:compare}
    \begin{pythoncode}
        ndigits = 6  # The number of digits we truncate the operand to
        x = np.random.rand()
        ys = np.random.rand(10000) + 0.542
        colnames = ['n', "type", "full", "rounded", "truncated"]
        df = pd.DataFrame(data=[[0, "value", x, myround(x, ndigits), truncate(x, ndigits)]],
                          columns=colnames)
        for n in (10, 100, 1000, 10000):
            a = reduce(mul, ys[:n], x)
            b = rounded_accumulate(ys[:n], ndigits, x)
            c = truncated_accumulate(ys[:n], ndigits, truncate(x, ndigits))
            diff = 1 - [a, b, c] / a
            new = pd.DataFrame(
                data=[[n, "value", a, b, c], [n, "diff", *diff]], columns=colnames)
            df = pd.concat([df, new])
        df = df.set_index(['n', "type"])
    \end{pythoncode}
\end{algorithm}

Here we give an example table of results, as shown in Tab. \ref{tab:compare}. Notice how the
deviation for the truncated case is growing $10$ times worse with every $10$ times increase
in floating point operations.

\begin{table}
    \centering
    \caption{An example of how the rounded and truncated results differ from the results of
        standard floating point numbers after $0$, $10$, $100$, $1000$ and $10000$
        multiplications.}
    \label{tab:compare}
    \begin{tabular}{@{}ccccc@{}}
        \toprule
        $n$                      & type  & full            & rounded         & truncated       \\
        \midrule
        $0$                      & value & $0.96056312$    & $0.96056300$    & $0.96056300$    \\
        \cmidrule{1-2}
        \multirow{2}{*}{$10$}    & value & $0.36782129$    & $0.36782242$    & $0.36781875$    \\
                                 & diff  & $0$             & $-0.00000309$   & $0.00000691$    \\
        \cmidrule{1-2}
        \multirow{2}{*}{$100$}   & value & $1.98537614$    & $1.98539378$    & $1.98526658$    \\
                                 & diff  & $0$             & $-0.00000889$   & $0.00005518$    \\
        \cmidrule{1-2}
        \multirow{2}{*}{$1000$}  & value & $49.15195379$   & $49.15357717$   & $49.12509557$   \\
                                 & diff  & $0$             & $-0.00003303$   & $0.00054643$    \\
        \cmidrule{1-2}
        \multirow{2}{*}{$10000$} & value & $7540.39925532$ & $7540.87648179$ & $7500.67631548$ \\
                                 & diff  & $0$             & $-0.00006329$   & $0.00526802$    \\
        \bottomrule
    \end{tabular}
\end{table}

Now we plot two histograms of the deviations for both cases, as shown in
Fig. \ref{fig:histogram}.
These distributions are obtained by sampling $100$ times with
large numbers of multiplications.
In the figure, the fractional difference means that
%
\begin{equation}
    \Delta = \frac{x_{\alpha} - x_\textnormal{exact}}{x_\textnormal{exact}},
\end{equation}
%
where $\alpha$ denotes either rounded or truncated case.
As you can see, the errors for the rounded case form a normal distribution around
$0$, and the errors for the truncated case gather around the right side of $0$.
This means the errors for the latter accumulate faster than the former for the same
number of multiplications.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{p1_q1_1}
    \caption{A histogram of the deviations for both the rounded (left) and
        truncated (right) cases.}
    \label{fig:histogram}
\end{figure}

We plot the average difference versus the number of multiplications for both the
rounding and truncation cases in Fig. \ref{fig:compare_rounding_truncation}.
And we do a sampling of these results for $n = 10$, $100$, $1000$, and $10000$ times,
the calculates their averages.

As you can see, for the rounded case, the errors look randomly distributed as a function
of $n$. It does show a little ``ups and downs'' trend but is not obvious.
On the other hand, the truncated values show an almost linear relation to $n$ (in log-log scale).
Since both the horizontal and vertical axes use logarithmic scales, the average
fractional differences ($r$) and $n$ should satisfy the power functions:
%
\begin{equation}
    r = a n^k
\end{equation}
%
where $a$ corresponds to the intercept (around $10^{-5}$), and $k$ corresponds to the
slope.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{p1_q1_2}
    \caption{Average difference versus the number of multiplications for both
        the rounding and truncation cases.}
    \label{fig:compare_rounding_truncation}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{p1_q1_3}
    \caption{Differences between average difference of the rounding and truncation cases.}
    \label{fig:diff_rounding_truncation}
\end{figure}

If we subtract the average fractional differences of the rounded case from the
truncated case, we can see a clear exponentially growing difference, as shown in
Fig. \ref{fig:diff_rounding_truncation}. This tells us the errors of the truncated
cases grow much faster than that of the rounded case, this is exactly what we observed
from Tab. \ref{tab:compare}.

\newpage
